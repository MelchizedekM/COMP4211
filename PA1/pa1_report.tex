\documentclass{article}
\usepackage{amssymb}
\usepackage{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{pifont}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption} 
\usepackage{subcaption} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{array}

\definecolor{codegray}{gray}{0.9}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single
}


\def\ra{\rightarrow}
\def\oo{\infty}
\def\l/{\backslash}
\def\0{\emptyset}
\def\b{\,\,\,}
\def\mm{{\mu^*}}
\def\hm{\mathcal{H}^s}
\def\vm{\nu^*}
\def\cui{\bigcup_{i=1}^\infty}
\def\cai{\bigcap_{i=1}^\infty}
\def\cuj{\bigcup_{j=1}^\infty}
\def\caj{\bigcap_{j=1}^\infty}
\def\sumj{\sum_{j=1}^\infty}
\def\sumi{\sum_{i=1}^\infty}
\def\sumn{\sum_{n=1}^\infty}
\def\px{\mathcal{P}_X}
\def\s{\mathcal{S}}
\def\a{\mathcal{A}}
\def\bs{\mathcal{B}}
\def\lm{\mathcal{L}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\Z{\mathbb{Z}}
\def\m{\mathcal{M}}
\def\rr{\Rightarrow}
\def\tf{$\Rightarrow$}
\def\f{\mathcal{F}}
\def\limn{\lim_{n \rightarrow \infty}}
\def\st{\text{s.t.}}
\def\sums{ \sum_{x \in \s}}

%Ricky Def
\def\baru{\bar{\mu}}
\def\Mbaru{\mathfrak{M}_{\baru}}


\title{COMP4211 PA1 Report} 
\author{Name: Ruiming Min; SID: 20827430; ITSC: rmin}
\date{\today}

\begin{document}

    
\maketitle

\section*{Part 1: Data Exploration and Preparation}

\subsection*{Q1.}
\begin{itemize}
    \item The training set contains 3539 samples(instances), each with 31 features (exclude two labels, `regression labels' and `classification labels') and 2 labels (`regression labels' and `classification labels').
    \item The `C0' to `C5', `C7' to `C13' and `C15' features are categorical, while the rest are numerical.
\end{itemize}

\subsection*{Q2.}

\subsubsection*{The missing values and the percentage of missing values for each feature:}
% draw a dable to show the missing values and the percentage of missing values
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        Feature & Missing Values & Percentage of Missing Values \\
        \hline
        C0 & 29 & 0.819441\% \\
        C4 & 49 & 1.384572\% \\
        C5 & 108 & 3.051710\% \\
        C8 & 82 & 2.317039\% \\
        C9 & 142 & 4.012433\% \\
        C11 & 158 & 4.464538\% \\
        C12 & 170 & 4.803617\% \\
        C13 & 120 & 3.390788\% \\
        C15 & 138 & 3.899407\% \\
        C17 & 148 & 4.181972\% \\
        C20 & 171 & 4.831873\% \\
        C22 & 16 & 0.452105\% \\
        C23 & 28 & 0.791184\% \\
        C25 & 144 & 4.068946\% \\
        C29 & 160 & 4.521051\% \\
        \hline
    \end{tabular}
    \newline
    \textbf{P.S.: If the feature is not listed in the table, then it has no missing values.}
\end{center}

\subsubsection*{The effect of missing values on the distribution of the features:}
Since the missing values may make the information less compheresive, which may lead more bias. 
These missing values may make the distribution of the features biased, 
and may also affect the performance of the model. 
Moreover, since some algorithms may not handle the missing values, these missing values may also affect the performance of the model.


\subsection*{Q3.}
\subsubsection*{Numerical:}
The `C6', `C20' and `C26' features are continuous. The rest are discrete. 
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Feature & Mean & Standard Deviation & Median & Range\\
        \hline
        First N-Feature (C6)  & 66.324696  & 6.619953 & 66.55& 47.5~95 \\
        Second N-Feature (C14)  & 23.204012 & 7.427675 & 20 & 17~70 \\
        Third N-Feature (C16)  & 0.683526 & 2.275012 & 0 & 0~20 \\
        \hline
    \end{tabular}
\end{center}

Boxplot for the three numerical features:
% import the pic from ./pic/boxplot.png
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/boxplot.png}
    \caption{Boxplot for the three numerical features}
\end{figure}

\subsubsection*{Categorical:}
Binary features indexes: C4, C8, C9, C10, C11, C12, C13, C15.\\
Nominal features indexes: C0, C1, C3, C7.\\ 
Ordinal features indexes: C2, C5.\\



\begin{table}[h!]
    \centering % 确保表格整体居中
    \begin{subtable}[t]{0.4\textwidth}
        \centering % 子表格内容居中
        \begin{tabular}{|c|c|}
            \hline
            Value & Count \\
            \hline
            single    & 3115 \\
            married  & 296 \\
            divorced & 75 \\
            facto union & 18 \\
            legally separated & 3 \\
            widower & 3\\
            \hline
        \end{tabular}
        \caption{\textbf{1st Categorical Feature:}} % 子表格标题
    \end{subtable}
    %\hfill % 使用 \hfill 来确保子表格之间有空间
    \begin{subtable}[t]{0.4\textwidth}
        \centering % 子表格内容居中
        \begin{tabular}{|c|c|}
            \hline
            Value & Count \\
            \hline
            second choice   & 2402 \\
            third choice & 457 \\
            fourth choice  & 247 \\
            fifth choice & 194 \\
            sixth choice & 125 \\
            seventh choice & 112 \\
            last choice  & 1 \\
            first choice  & 1 \\
            \hline
        \end{tabular}
        \caption{\textbf{3rd Categorical Feature:}} % 子表格标题
    \end{subtable}

\end{table}

\begin{table}[h!]
        \centering % 子表格内容居中
        \small
    \begin{tabular}{|c|c|}
        \hline
        Value & Count \\
        \hline
        1st phase - general contingent   & 1351 \\
        2nd phase - general contingent & 708 \\
        Over 23 years old  & 630 \\
        Change of course & 253 \\
        Technological specialization diploma holders & 160 \\
        Holders of other higher courses & 109 \\
        3rd phase - general contingent  & 105 \\
        Transfer & 67 \\
        Change of institution/course & 45 \\
        Short cycle diploma holders  & 30 \\
        1st phase - special contingent (Madeira Island) & 29 \\
        International student (bachelor) & 26 \\
        1st phase - special contingent (Azores Island) & 14 \\
        Ordinance No. 854-B/99 & 7 \\
        Ordinance No. 612/93 & 2 \\
        Ordinance No. 533-A/99, item b3 (Other Institution) & 1 \\
        Ordinance No. 533-A/99, item b2) (Different Plan) & 1 \\
        Change of institution/course (International) & 1 \\
        \hline
        
    \end{tabular}
    \caption{\textbf{2nd Categorical Feature:}}  
\end{table}

% import the pic from ./pic/categorical.png
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/categorical.png}
    \caption{Barplot for the three categorical features}
\end{figure}

\newpage

\subsection*{Q4.}
\subsubsection*{Detection:}
Z-Score:\\
\begin{itemize}
    \item Outliers Index of First N-Feature (7-th): 59,  176,  207,  342,  971, 1156, 1840, 2272, 2282, 2447, 2448, 2535, 2624, 3259, 3300, 3373, 3487, 3490
    \item Outliers Index of Second N-Feature (15-th): 99,  252,  278,  280,  291,  394,  403,  432,  513,  553,  607, 
    687,  695,  719,  722,  790,  985,  987, 1018, 1036, 1214, 1233,
    1244, 1266, 1316, 1388, 1408, 1455, 1508, 1512, 1515, 1573, 1581,
    1647, 1691, 1725, 1756, 1782, 1891, 1892, 1906, 1966, 1972, 2020,
    2119, 2125, 2126, 2140, 2178, 2195, 2260, 2317, 2406, 2450, 2509,
    2557, 2613, 2650, 2674, 2702, 2731, 2787, 2827, 2831, 2856, 2895,
    2930, 3048, 3069, 3075, 3077, 3111, 3112, 3126, 3187, 3210, 3327,
    3343, 3412
    \item Outliers Index of Third N-Feature (17-th):18,   33,   83,  150,  152,  173,  278,  337,  362,  391,  424,
    474,  493,  508,  513,  516,  522,  526,  549,  552,  555,  586,
    609,  643,  657,  683,  771,  783,  851,  909,  912,  916,  947,
   1106, 1112, 1174, 1180, 1189, 1240, 1243, 1244, 1298, 1299, 1316,
   1364, 1412, 1428, 1429, 1440, 1463, 1507, 1524, 1541, 1576, 1581,
   1589, 1602, 1632, 1647, 1699, 1789, 1797, 1843, 1847, 1855, 1876,
   1884, 1931, 1965, 1986, 2015, 2054, 2075, 2100, 2122, 2224, 2242,
   2279, 2335, 2339, 2377, 2430, 2455, 2459, 2535, 2555, 2625, 2674,
   2676, 2684, 2698, 2716, 2757, 2764, 2804, 2814, 2821, 2827, 2913,
   2920, 2950, 3045, 3050, 3053, 3064, 3068, 3086, 3273, 3354, 3376,
   3448, 3470, 3500, 3528
\end{itemize}

IQR:\\

\begin{itemize}
    \item Outliers Index of First N-Feature (7-th): 18, 20, 59, 77, 93, 101, 114, 171, 176, 201, 207, 278, 307, 322, 323, 330, 342, 382, 387, 472, 498, 508, 523, 537, 542, 549, 553, 583, 607, 635, 721, 745, 764, 776, 786, 795, 796, 803, 807, 821, 848, 912, 971, 987, 1021, 1031, 1036, 1054, 1156, 1182, 1194, 1217, 1224, 1235, 1244, 1261, 1284, 1307, 1333, 1386, 1390, 1407, 1451, 1482, 1485, 1516, 1571, 1602, 1628, 1629, 1669, 1683, 1711, 1747, 1840, 1851, 1861, 1871, 1884, 1890, 1892, 1932, 1944, 1966, 1972, 1985, 2026, 2047, 2050, 2051, 2177, 2178, 2197, 2256, 2263, 2272, 2282, 2293, 2333, 2373, 2391, 2411, 2413, 2415, 2422, 2447, 2448, 2535, 2539, 2598, 2616, 2622, 2624, 2675, 2737, 2756, 2763, 2773, 2803, 2809, 2816, 2835, 2915, 2968, 3003, 3043, 3072, 3085, 3144, 3153, 3194, 3206, 3236, 3259, 3260, 3272, 3278, 3300, 3317, 3332, 3370, 3373, 3427, 3431, 3487, 3490, 3536.
    \item Outliers Index of Second N-Feature (15-th): 0, 3, 40, 47, 49, 85, 99, 117, 133, 136, 160, 173, 215, 236, 244, 252, 255, 278, 280, 291, 305, 307, 309, 311, 332, 362, 374, 390, 391, 394, 403, 416, 419, 430, 432, 465, 468, 474, 480, 492, 500, 507, 513, 515, 516, 521, 523, 525, 526, 545, 552, 553, 560, 564, 598, 600, 605, 607, 633, 635, 638, 644, 653, 668, 675, 687, 695, 719, 722, 726, 729, 730, 737, 739, 754, 764, 768, 771, 790, 806, 828, 848, 851, 864, 881, 985, 987, 1018, 1029, 1032, 1036, 1051, 1105, 1114, 1124, 1140, 1147, 1156, 1182, 1214, 1217, 1233, 1239, 1244, 1246, 1248, 1254, 1264, 1266, 1287, 1305, 1316, 1351, 1368, 1370, 1377, 1380, 1381, 1388, 1398, 1406, 1408, 1421, 1453, 1455, 1462, 1479, 1481, 1504, 1508, 1512, 1515, 1525, 1569, 1573, 1581, 1586, 1591, 1592, 1602, 1623, 1637, 1644, 1647, 1648, 1654, 1663, 1671, 1673, 1678, 1691, 1704, 1717, 1725, 1732, 1745, 1749, 1754, 1755, 1756, 1765, 1782, 1784, 1789, 1791, 1795, 1799, 1824, 1836, 1855, 1861, 1867, 1886, 1891, 1892, 1906, 1924, 1931, 1949, 1966, 1969, 1971, 1972, 1974, 1985, 1996, 1997, 2020, 2042, 2047, 2053, 2094, 2101, 2106, 2117, 2119, 2122, 2125, 2126, 2130, 2140, 2144, 2171, 2177, 2178, 2189, 2195, 2197, 2198, 2225, 2226, 2230, 2243, 2260, 2270, 2284, 2290, 2316, 2317, 2332, 2369, 2373, 2377, 2386, 2398, 2406, 2410, 2411, 2420, 2442, 2449, 2450, 2459, 2466, 2481, 2491, 2502, 2509, 2529, 2535, 2557, 2559, 2567, 2571, 2574, 2591, 2595, 2611, 2613, 2634, 2650, 2663, 2669, 2674, 2686, 2702, 2714, 2731, 2751, 2758, 2764, 2773, 2787, 2803, 2827, 2829, 2831, 2842, 2851, 2853, 2856, 2857, 2858, 2864, 2865, 2885, 2895, 2906, 2909, 2930, 2943, 2952, 2963, 2965, 2995, 2997, 3003, 3023, 3037, 3048, 3069, 3070, 3072, 3075, 3076, 3077, 3084, 3092, 3095, 3102, 3106, 3111, 3112, 3118, 3126, 3144, 3148, 3159, 3163, 3172, 3187, 3207, 3210, 3224, 3236, 3264, 3284, 3289, 3309, 3323, 3327, 3335, 3343, 3376, 3380, 3382, 3388, 3393, 3397, 3399, 3412, 3415, 3431, 3434, 3448, 3463, 3484, 3499, 3518, 3523
    \item Outliers Index of Third N-Feature (17-th): 5, 12, 14, 17, 18, 27, 33, 44, 48, 53, 58, 66, 74, 83, 86, 92, 101, 121, 126, 144, 150, 152, 156, 160, 164, 166, 169, 173, 174, 200, 210, 223, 243, 246, 253, 267, 278, 288, 298, 304, 317, 322, 324, 327, 335, 336, 337, 353, 358, 362, 391, 404, 409, 412, 424, 429, 430, 438, 444, 456, 472, 474, 479, 493, 495, 508, 509, 513, 516, 522, 526, 534, 549, 552, 555, 561, 564, 586, 598, 605, 609, 612, 614, 618, 635, 637, 643, 649, 654, 657, 660, 662, 675, 683, 687, 721, 726, 739, 744, 754, 762, 765, 766, 770, 771, 781, 783, 798, 801, 811, 812, 814, 834, 843, 847, 851, 860, 870, 909, 911, 912, 914, 916, 946, 947, 958, 970, 985, 987, 989, 992, 1001, 1020, 1021, 1030, 1044, 1051, 1073, 1091, 1095, 1101, 1106, 1112, 1113, 1122, 1139, 1145, 1156, 1160, 1174, 1179, 1180, 1183, 1189, 1197, 1233, 1240, 1243, 1244, 1253, 1277, 1289, 1298, 1299, 1304, 1305, 1311, 1313, 1316, 1323, 1361, 1362, 1364, 1366, 1381, 1395, 1404, 1412, 1419, 1428, 1429, 1440, 1458, 1463, 1464, 1467, 1472, 1490, 1494, 1497, 1504, 1505, 1507, 1509, 1524, 1537, 1541, 1568, 1573, 1576, 1581, 1589, 1591, 1595, 1602, 1603, 1623, 1632, 1638, 1644, 1645, 1647, 1652, 1655, 1659, 1668, 1699, 1708, 1714, 1720, 1722, 1735, 1748, 1766, 1772, 1776, 1784, 1785, 1789, 1791, 1794, 1797, 1805, 1806, 1808, 1812, 1814, 1815, 1821, 1822, 1827, 1841, 1843, 1847, 1855, 1858, 1861, 1863, 1867, 1876, 1884, 1894, 1897, 1898, 1904, 1921, 1922, 1926, 1931, 1950, 1965, 1966, 1974, 1986, 1987, 2002, 2004, 2005, 2012, 2013, 2015, 2016, 2031, 2039, 2054, 2055, 2075, 2080, 2086, 2100, 2106, 2109, 2122, 2160, 2171, 2172, 2173, 2189, 2211, 2214, 2224, 2225, 2235, 2236, 2242, 2244, 2248, 2260, 2267, 2279, 2303, 2319, 2326, 2329, 2330, 2333, 2334, 2335, 2339, 2341, 2351, 2356, 2361, 2376, 2377, 2383, 2386, 2395, 2398, 2411, 2422, 2430, 2432, 2434, 2436, 2455, 2459, 2471, 2480, 2481, 2494, 2499, 2504, 2511, 2519, 2534, 2535, 2555, 2571, 2576, 2581, 2586, 2598, 2604, 2625, 2635, 2644, 2650, 2665, 2674, 2676, 2684, 2686, 2689, 2692, 2698, 2716, 2729, 2735, 2742, 2743, 2756, 2757, 2764, 2773, 2782, 2787, 2793, 2801, 2804, 2806, 2814, 2816, 2821, 2823, 2827, 2832, 2856, 2860, 2866, 2876, 2891, 2906, 2913, 2920, 2930, 2947, 2950, 2994, 2996, 3009, 3020, 3024, 3032, 3045, 3050, 3053, 3054, 3060, 3064, 3065, 3068, 3085, 3086, 3104, 3137, 3156, 3160, 3171, 3179, 3209, 3215, 3249, 3251, 3273, 3278, 3283, 3284, 3285, 3286, 3295, 3300, 3301, 3303, 3307, 3309, 3315, 3325, 3335, 3346, 3354, 3360, 3375, 3376, 3388, 3393, 3399, 3405, 3416, 3432, 3436, 3437, 3444, 3448, 3468, 3470, 3484, 3485, 3493, 3499, 3500, 3512, 3528, 3534
\end{itemize}

\subsubsection*{Consideration:}
Since too many outliers may influence the mean and standard deviation, and the distribution of the data. The outliers may also affect the performance of the model.

\subsection*{Q5.}
\subsubsection*{Feature correlation:}

The following is the heatmap (Figure \ref{fig:heatmap}) for the correlation matrix of the numerical features:
% import the pic from ./pic/heatmap.png
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/heatmap.png}
    \caption{Heatmap for the correlation matrix}
    \label{fig:heatmap}
\end{figure}


\subsubsection*{Insights:}
The following pairs of features are highly correlated (correlation coefficient $> 0.7$):\\
Features C16 and C17 have a correlation of 0.76\\
Features C20 and C26 have a correlation of 0.84\\
Features C16 and C22 have a correlation of 0.94\\
Features C23 and C25 have a correlation of 0.70\\
Features C19 and C20 have a correlation of 0.71\\
Features C19 and C25 have a correlation of 0.90\\
Features C17 and C22 have a correlation of 0.75\\
Features C25 and C26 have a correlation of 0.77\\
Features C17 and C23 have a correlation of 0.94\\
Features C17 and C19 have a correlation of 0.77\\
Features C19 and C23 have a correlation of 0.73\\
Features C18 and C24 have a correlation of 0.78\\

Since these features are highly correlated, we may consider to remove some of them to avoid the multicollinearity.
We can also use PCA, UMAP or tSEN to reduce the dimensionality of these features.

\subsection*{Q6.}
\subsubsection*{Data Processing Steps:}
\begin{itemize}
    \item Fill the missing values with the mode of the feature.
    \item Reproduce the categorical features with one-hot encoding.
    \item Remove the outliers or balence the influence of the outliers.
    \item Standardize the numerical features.
    \item Integrate the highly correlated features to avoid multicollinearity.
\end{itemize}

\subsubsection*{Challenges:}

If we can integrate the highly correlated features, we may not only avoid the multicollinearity but also reduce the dimensionality of the data. However, the integration of the features may also lead to the loss of information.

If we can balence the weight of the outliers, we may not only reduce the negative influence of the outliers but also keep the information of the outliers. However, the balence of the outliers may also lead to the loss of information.

\newpage

\section*{Part 2:  Data Preprocessing Techniques}

\subsection*{Q7.}

\textbf{After testing these methods, we can find the following feature distribution and model performance:}

\begin{itemize}
    \item Mean - this method will fill the missing values with the mean of the feature column. This method will make the data more centered. However, this method may be influenced by the outliers. Therefore, this method may not be suitable for the data with outliers.
    \item Median - this method will fill the missing values with the median of the feature column. This method will make the data more robust to the outliers. However, this method may not be suitable for the data with a skewed distribution.
    \item Mode - this method will fill the missing values with the most frequent value of the feature column. This method is suitable for categorical or discrete data to maintain representativeness, but it may not be ideal for continuous data due to potential distortions in the distribution
    \item Constant - this method will fill the missing values with a constant, which is chosen by us. This method may be suitable for the data with a specific meaning of the missing values. However, this method may have some potential risks, such as the distortion of the distribution.
\end{itemize}

\textbf{After testing these methods, we can find what imputation should be used and when as following:}

\begin{itemize}
    \item If the feature column has a normal distribution and the number of outliers is small, we may consider to use the Mean imputation.
    \item If the feature column has a skewed distribution or have too many outliers, we may consider to use the Median imputation.
    \item If the feature column is a categorical or discrete feature, we may consider to use the Mode imputation.
    \item If the feature column has a specific meaning of the missing values, we may consider to use the Constant imputation.
\end{itemize}

\subsection*{Q8.}

From the table \ref{tab:Standardize}, we can find the following insights:

\begin{itemize}
    \item Comparing with other methods, Standard Scaler could make the mean of the feature column to be 0 and the standard deviation to be 1. This method make the data to be more Gaussian-like.
    \item Comparing with other methods, Min-Max Scaler could make the data to be in the range of 0 to 1. This method could afine the data to a fixed range. However, this method may be influenced by the outliers, which may make most of the values to a norrow range.
    \item Comparing with other methods, Robust Scaler could re-build the weight of the outliers. This method could make the data to be less influenced by the outliers.
\end{itemize}


\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Index & Original & Standard Scaler & Min-Max Scaler & Robust Scaler \\
        \hline
        0  & 65.00 & -0.200135 & 0.368421 & -0.206667 \\
        1  & 65.00 & -0.200135 & 0.368421 & -0.206667 \\
        2  & 59.50 & -1.031074 & 0.252632 & -0.940000 \\
        3  & 66.55 &  0.034039 & 0.401053 & 0.000000 \\
        4  & 71.00 &  0.706344 & 0.494737 & 0.593333 \\
        5  & 70.00 &  0.555264 & 0.473684 & 0.460000 \\
        6  & 57.50 & -1.333234 & 0.210526 & -1.206667 \\
        7  & 65.50 & -0.124595 & 0.378947 & -0.140000 \\
        8  & 70.00 &  0.555264 & 0.473684 & 0.460000 \\
        9  & 80.00 &  2.066063 & 0.684211 & 1.793333 \\
        \hline
    \end{tabular}
    \caption{ the first numerical feature column of the first 10 samples before and after processing}
    \label{tab:Standardize}
\end{table}


\subsection*{Q9.}

The table \ref{tab:Categorical} shows the first 10 samples of the first categorical feature column before and after processing:

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Index & Original & OneHotEncoder & OrdinalEncoder \\
        \hline
        0  & divorced & [1. 0. 0. 0. 0. 0. 0.] & 0  \\
        1  & single & [0. 0. 0. 0. 1. 0. 0.] & 4  \\
        2  & single & [0. 0. 0. 0. 1. 0. 0.] & 4 \\
        3  & married &  [0. 0. 0. 1. 0. 0. 0.] & 3  \\
        4  & single &  [0. 0. 0. 0. 1. 0. 0.] & 4 \\
        5  & single &  [0. 0. 0. 0. 1. 0. 0.] & 4  \\
        6  & single & [0. 0. 0. 0. 1. 0. 0.] & 4  \\
        7  & single & [0. 0. 0. 0. 1. 0. 0.] & 4  \\
        8  & single &  [0. 0. 0. 0. 1. 0. 0.] & 4  \\
        9  & single &  [0. 0. 0. 0. 1. 0. 0.] & 4  \\
        \hline
    \end{tabular}
    \caption{ the first categorical feature column of the first 10 samples before and after processing}
    \label{tab:Categorical}
\end{table}

Scenarios:

\begin{itemize}
    \item If the categorical feature has a natural order, we may consider to use the OrdinalEncoder to encode the feature.
    \item If the categorical feature has no natural order, we may consider to use the OneHotEncoder to encode the feature.
\end{itemize}

\subsection*{Q10.}

\subsubsection*{VarianceThreshold:}

This method could remove the features with a variance lower than a threshold, which means this method will remove those variabls almost never change. This method will reduce the dimension of input variables and lead less possibility of overfitting. 
In this data set, it removed 2 features. 
After comparing the selected features with the original features, 
we find that C15 and C9 are removed if we use OE to encode the categorical features.

\subsubsection*{SelectKBest:}

This method could select the top k features with the highest scores. This method will reduce the dimension of input variables and lead less possibility of overfitting.
In this data set, it selected 15 features.
After comparing the selected features with the original features,
we find that C6, C14, C17, C20, C23, C25, C26, C0, C1, C13, C15, C4, C5, C7, C8 are selected if we use OE to encode the categorical features.

\subsection*{Q11.}

Since from the heatmap (Figure \ref{fig:heatmap}), we could find the fact that there are lots of highly correlated features, we may consider to use PCA to reduce the dimensionality of the data and improve the performance of the model.

From the variable importance line (Figure \ref{fig:pca}), we could find the fact that the first 6 principal components could explain around 90\% of the variance of the data. So we may consider to use the first 6 principal components to represent the data.


% import the pic from ./pic/pca.png
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/pca.png}
    \caption{The explained variance of the principal components}
    \label{fig:pca}
\end{figure}

\newpage

\section*{Part 3: Regression}

\subsection*{Q12.}

The table \ref{tab:Regression} shows the performance of the three models:

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|}
        \hline
        Variable(s) & $R^2$ & Validation $R^2$ & MSE  \\
        \hline
        PCA1 of SS+OE data &  0.0018695316290635589 & 0.00494776279640774 & 0.004840345917165619 \\
        PCA2 of SS+OE data & 0.0007037263251143955 & 0.005404304670441729 &  0.004838125108535304 \\
        PCA10 of SS+OE data & 0.2805220334276459 & 0.32358168954322797 & 0.003290378620238816  \\
        PCA6 of SS+OE data & 0.0031871125899599617 & 0.006365959135546939 & 0.004833447223204402 \\
        PCA4 of SS+OE data & 0.00938766462088203 & 0.005735289304080737 & 0.004836515062288304\\
        PCA5 of SS+OE data & 0.004886260271825815 & 0.0072159044603422595 & 0.004829312737366504 \\
        All used features & 0.3029115372754613 & 0.34255926917072277 & 0.0031980638184292347  \\
        \hline
    \end{tabular}
    \caption{The performance of the three models}
    \label{tab:Regression}
\end{table}

Since the $R^2$ reflect the proportion of the variance in the dependent variable that is predictable from the independent variable(s). The we can conclude the higher the $R^2$, the higher correlation between the dependent variable and the independent variable(s).

\subsection*{Q13.}

From the table \ref{tab:Regression}, we can find the $R^2$ and MSE satifies a negative correlation, i.e., the higher the $R^2$, the lower the MSE. 


\subsection*{Q14.}

\subsubsection*{Meaning of Binary Features:}

In the encoded binary features, there are commonly two values, 0 and 1. 
Then the sign weight for it could means that if we switch this feature from the first kind (represented by 0) to the second kind (represented by 1), the dependent variable will be influenceed positively or negatively.
Moreover, the absolute value of the weight could also means the importance of the feature, i.e., the higher the absolute value of the weight, the higher the influence of the feature could make to the dependent variable.

\subsubsection*{Repeating Q12, Q13 using a categorical feature:}

After fitting a linear regression model using a categorical feature, we find that:

\begin{itemize}
    \item The $R^2$ is 0.002696133285668423, which means that the model is not good.
    \item The MSE is 0.004833949055816101, which is relatively high comparing to the MSE of the other models.
    \item The validation $R^2$ score is 0.00626279509076233.
\end{itemize}

Nomrally, there are two ways to embed the categorical features into the linear regression model: one-hot encoding and ordinal encoding. If the data itself has a natural order, we may consider to use the ordinal encoding. Otherwise, we may consider to use the one-hot encoding. 

\subsection*{Q15.}

The table \ref{tab:Feedforward} and figure \ref{fig:feedforward} shows the performance of the feedforward neural network model:

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        H values & $R^2_1$ & $R^2_2$ & $R^2_3$ & mean $R^2$ & Std($R^2$) & $T_1$ & $T_2$ & $T_3$ & mean $T$ & Std($T$) \\
        \hline
        1 & 0.07566 & 0.05976 &  -0.00093 & 0.0448 & 0.0330 & 1.2492 & 1.2492 & 0.2607  & 0.2607 & 0.2607\\
        8 & 0.29031 & 0.26985 & 0.24427 & 0.2681 & 0.0188 & 0.5579 & 0.8106 & 1.3534 & 0.9073 & 0.3319 \\
        32 & 0.35848 & 0.25432 & 0.20716 & 0.2733 & 0.0632 & 1.4509 & 1.2168 & 1.5971 & 1.4216 & 0.1566 \\
        128 & 0.35523 & 0.30468& 0.39429 & 0.3514 & 0.0367 & 4.3852 & 1.4458 & 1.3225 & 1.4063 & 0.0593 \\
        \hline
    \end{tabular}
    \caption{The performance of the feedforward neural network model}
    \label{tab:Feedforward}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/R2_Score_vs_Hidden_Units.png}
    \caption{The performance of the feedforward neural network model}
    \label{fig:feedforward}
\end{figure}


\subsection*{Q16.}

From the table \ref{tab:Feedforward}, we can find the best performance of the feedforward neural network model is when the number of hidden layers is 128.
The best model has the highest validation $R^2$ equll to 0.39429, which means the model has the highest correlation between the dependent variable and the independent variable(s).
However, the training time of the best model is 3.9308, which is relatively high comparing to the training time of the linear models.



\newpage

\section*{Part 4: Classification}

\subsection*{Q17.}

The teble \ref{tab:complete_model_performance} the performance of the models and the ROC curve (Figure \ref{fig:ROC}) shows the performance of the last model.
According to the user guide, when we use the command "optimal" the learning rate, $\iterate_0$, will be optimized by the model itself. Therefore, the change of it will not influence the performance of the model, which is consistent with the table \ref{tab:complete_model_performance}.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{ccccccc}
    \toprule
    Learning Rate & $\eta_0$ & Tolerance & Avg. Training Time (s) & Avg. F1 Score & Avg. Test Acc. & Avg. Train Acc. \\
    \midrule
    constant & 0.02 & 0.001 & 0.0038 $\pm$ 0.0008 & 0.823 $\pm$ 0.010 & 0.826 $\pm$ 0.011 & 0.792 $\pm$ 0.014 \\
    constant & 0.01 & 0.001 & 0.0044 $\pm$ 0.0004 & 0.850 $\pm$ 0.001 & 0.855 $\pm$ 0.001 & 0.821 $\pm$ 0.002 \\
    constant & 0.02 & 0.01 & 0.0036 $\pm$ 7.19e-05 & 0.799 $\pm$ 0.027 & 0.802 $\pm$ 0.028 & 0.762 $\pm$ 0.036 \\
    constant & 0.01 & 0.01 & 0.0035 $\pm$ 4.41e-05 & 0.823 $\pm$ 0.006 & 0.826 $\pm$ 0.006 & 0.787 $\pm$ 0.019 \\
    optimal & 0.02 & 0.001 & 0.0171 $\pm$ 0.0038 & 0.825 $\pm$ 0.008 & 0.830 $\pm$ 0.006 & 0.798 $\pm$ 0.008 \\
    optimal & 0.01 & 0.001 & 0.0171 $\pm$ 0.0038 & 0.825 $\pm$ 0.008 & 0.830 $\pm$ 0.006 & 0.798 $\pm$ 0.008 \\
    optimal & 0.02 & 0.01 & 0.0151 $\pm$ 0.0020 & 0.813 $\pm$ 0.010 & 0.817 $\pm$ 0.012 & 0.787 $\pm$ 0.013 \\
    optimal & 0.01 & 0.01 & 0.0154 $\pm$ 0.0022 & 0.813 $\pm$ 0.010 & 0.817 $\pm$ 0.012 & 0.787 $\pm$ 0.013 \\
    \bottomrule
    \end{tabular}
    \caption{Model Performance Across Different Parameters  (all the data satifies this $\overline{x}$ $\pm$ $\sigma$ formular)}
    \label{tab:complete_model_performance}
\end{table}
    
% import the pic from ./pic/ROC_curve_set7.png


\subsection*{Q18.}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/ROC_curve_set7.png}
    \caption{ROC curve for the best model}
    \label{fig:ROC}
\end{figure}

From the figure \ref{fig:ROC}, we can find the AUC of the ROC curve is 0.88. 

Since if we need to make sure the balence of false positive rate and false negative rate to avoid the bias because of the inbalance of positive data and negative data, we may consider to use the AUC of the ROC curve to evaluate the performance of the model. The higher the AUC, the better the performance of the model.


    
\subsection*{Q19.}

From the table \ref{tab:complete_model_performance}, as the learning rate increases, the training time decreases, the F1 score and the testing accuracy decrease, and the training accuracy decreases.

\begin{table}[htbp]
    \centering
    \begin{tabular}{cccccc}
    \toprule
    Learning Rate & Training Time (s) & F1 Score & Testing Accuracy & Training Accuracy \\
    \midrule
    0.05 & 0.0026 & 0.765958 & 0.774011 & 0.723773 \\
    0.04 & 0.0036 & 0.799790 & 0.806497 & 0.759449 \\
    0.03 & 0.0036 & 0.782188 & 0.788136 & 0.754504 \\
    0.02 & 0.0047 & 0.816410 & 0.817797 & 0.778877 \\
    0.01 & 0.0052 & 0.847873 & 0.853107 & 0.820205 \\
    \bottomrule
    \end{tabular}
    \caption{Model Performance with Constant Learning Rate}
    \label{tab:model_performance_constant_lr}
\end{table}

\subsection*{Q20.}

The table \ref{tab:detailed_model_performance} shows the detailed performance of the model across different hidden units.

\begin{table}[htbp]
    \centering
    \begin{tabular}{ccccccc}
    \toprule
    H &  Train Time & Accuracy & F1 & Train Time ($\overline{x}$ $\pm$ $\sigma$) & Accuracy Rate ($\overline{x}$ $\pm$ $\sigma$)& F1 ($\overline{x}$ $\pm$ $\sigma$)\\
    \midrule
    \multirow{3}{*}{1} & 0.0436 & 0.3037 & 0.1415 & \multirow{3}{*}{0.4346 $\pm$ 0.0253} & \multirow{3}{*}{0.4346 $\pm$ 0.1851} & \multirow{3}{*}{0.2849 $\pm$ 0.2028} \\
                       & 0.0929 & 0.3037 & 0.1415 & & \\
                       & 0.1007 & 0.6963 & 0.5717 & & \\
    \midrule
    \multirow{3}{*}{8} & 0.3240 & 0.8588 & 0.8520 & \multirow{3}{*}{0.4346 $\pm$ 0.0887} & \multirow{3}{*}{0.8395 $\pm$ 0.0156} & \multirow{3}{*}{0.8312 $\pm$ 0.0178} \\
                       & 0.1523 & 0.8390 & 0.8332 & & \\
                       & 0.1230 & 0.8206 & 0.8085 & & \\
    \midrule
    \multirow{3}{*}{32} & 0.3688 & 0.8630 & 0.8571 & \multirow{3}{*}{0.4346 $\pm$ 0.0169} & \multirow{3}{*}{0.8611 $\pm$ 0.0070} & \multirow{3}{*}{0.8551 $\pm$ 0.0085} \\
                        & 0.3927 & 0.8517 & 0.8438 & & \\
                        & 0.4100 & 0.8686 & 0.8643 & & \\
    \midrule
    \multirow{3}{*}{128} & 0.7175 & 0.8602 & 0.8553 & \multirow{3}{*}{0.4346 $\pm$ 0.0702} & \multirow{3}{*}{0.8616 $\pm$ 0.0031} & \multirow{3}{*}{0.8567 $\pm$ 0.0014} \\
                         & 0.6443 & 0.8588 & 0.8563 & & \\
                         & 0.8156 & 0.8658 & 0.8586 & & \\
    \bottomrule
    \end{tabular}
    \caption{Detailed Model Performance Across Trials with Different Hidden Units}
    \label{tab:detailed_model_performance}
\end{table}
    
\subsection*{Q21.}

The figure \ref{fig:Performance_vs_H} shows the performance of the model across different hidden units.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./pic/Combined_Performance_vs_H.png}
    \caption{Performance of the model across different hidden units}
    \label{fig:Performance_vs_H}
\end{figure}

\subsubsection*{Possible Reason:}

One of the possible reasons for the gap between the testing accuracy rate and F score is that the testing accuracy rate is not sensitive to the imbalance of the positive data and the negative data, while the F1 score is sensitive to the imbalance of the positive data and the negative data.

\subsection*{Q22.}

From the table \ref{tab:detailed_model_performance}, we can find the best performance of the model has accuracy rate 0.8616 $\pm$ 0.0031 and F1 score 0.8567 $\pm$ 0.0014 when the number of hidden units is 128. This is higher than the Logistic Regression model, which has accuracy rate 0.855 $\pm$ 0.001 and F1 score 0.850 $\pm$ 0.001.
However, the training time of the best model is 0.4346 $\pm$ 0.0702, which is relatively high comparing to the training time of the Logistic Regression model.

\subsection*{Q23.}

From the figure \ref{tab:complete_model_performance}, we can trivailly find that as the hidden units increases the training time, the F1 score, and the testing accuracy increase.
One of the possible reasons could be that the more hidden units, the more complex the model, and the more complex the model, the more the model could fit the data.
Also, due to the complexity of the model, we may need more time to train the model.

\newpage

\section*{Part 5: Performance Enhancement}

\subsection*{Q24.}
\begin{itemize}
    \item Combination A - Classification - Accuracy: 0.8502824858757062, F1 Score: 0.893574297188755
    \item Combination A - Regression - R2 Score: 0.30468445326826177, MSE: 0.003382302598728934
\end{itemize}

\subsection*{Q25.}
\begin{itemize}
    \item Combination B - Classification - Accuracy: 0.8559322033898306, F1 Score: 0.9009708737864077
    \item Combination B - Regression - R2 Score: 0.2979528069556907, MSE: 0.0034150481125086187
\end{itemize}

\subsection*{Q26.}


I apply a series of data preprocessing steps before fitting the data to neural network models. These steps are differentiated for classification and regression tasks as follows:

\subsubsection*{Classification Pipeline}

\begin{lstlisting}
Numerical Features Preprocessing:
    - Imputation: Missing values are imputed using median for each column.
    - Scaling: Features are scaled using StandardScaler.

Ordinal Features Preprocessing:
    - Imputation: Missing values are replaced with an unique constant.
    - Encoding: OrdinalEncoder encodes categories as integers, handling unknown categories by assigning -1.

Nominal Features Preprocessing:
    - Imputation: Missing values are replaced with the most frequent value.
    - Encoding: OneHotEncoder is used for one-hot encoding, handling infrequent categories if they exist.

\end{lstlisting}

\subsubsection*{Regression Pipeline}

\begin{lstlisting}
Numerical Features Preprocessing:
    - Imputation: Missing values are imputed with the median.
    - Scaling: MinMaxScaler scales features to a range between 0 and 1.
    - Dimensionality Reduction: PCA reduces the data to 10 principal components.

Ordinal Features Preprocessing:
    - Imputation: Missing values are replaced with the most frequent value.
    - Encoding: OrdinalEncoder encodes categories as integers, handling unknown categories by assigning -1.

Nominal Features Preprocessing:
    - Imputation: Missing values are replaced with the most frequent value.
    - Encoding: OneHotEncoder is used for one-hot encoding, handling infrequent categories if they exist.

\end{lstlisting}

\begin{itemize}
\item Combination C - Classification - Accuracy: 0.8587570621468926, F1 Score: 0.9034749034749034
\item Combination C - Regression - R2 Score: 0.3743917925230793, MSE: 0.00304321724989664
\end{itemize}

\subsection*{Q27.}


The hyperparameter tuning was executed employing a grid search strategy on a three-hidden-layer feedforward neural network, considering the following hyperparameter grid:

\subsubsection*{Explored Hyperparameters:}
\begin{itemize}
    \item Hidden Layer Sizes: (128, 128, 128)
    \item Activation Functions: tanh, relu
    \item Solvers: sgd, adam
    \item Alpha: 0.0001, 0.001, 0.01
    \item Learning Rates: constant, adaptive
\end{itemize}

These hyperparameters will give us a total of 24 different combinations to explore. 

The grid search yielded the following optimal configuration and performance metrics:

\subsubsection*{Optimal Hyperparameters:}
\begin{itemize}
    \item Activation Function: tanh
    \item Alpha: 0.01
    \item Hidden Layer Sizes: (128, 128, 128)
    \item Learning Rate: constant
    \item Solver: adam
\end{itemize}

\textbf{Performance Metrics:}
\begin{itemize}
    \item Best Cross-Validation Accuracy: 0.8671814334947433
    \item Test Set Accuracy: 0.8983050847457628
    \item F1 Score: 0.8961675425899287
\end{itemize}

An evaluation of the hyperparameter space revealed the top five configurations based on mean validation accuracy, detailed as follows:

\begin{table}[h!]
    \begin{tabular}{@{}clccc@{}}
        \toprule
        Rank & Parameters & Mean Validation Accuracy & Std. Deviation \\
        \midrule
        1 & \begin{tabular}[c]{@{}l@{}}Activation: tanh, Alpha: 0.0001, \\ Hidden Layers: (128, 128, 128), \\ Learning Rate: constant, Solver: sgd\end{tabular} & 0.8534 & 0.0236 \\
        2 & \begin{tabular}[c]{@{}l@{}}Activation: tanh, Alpha: 0.0001, \\ Hidden Layers: (128, 128, 128), \\ Learning Rate: constant, Solver: adam\end{tabular} & 0.8668 & 0.0119 \\
        3 & \begin{tabular}[c]{@{}l@{}}Activation: tanh, Alpha: 0.0001, \\ Hidden Layers: (128, 128, 128), \\ Learning Rate: adaptive, Solver: sgd\end{tabular} & 0.8583 & 0.0142 \\
        4 & \begin{tabular}[c]{@{}l@{}}Activation: tanh, Alpha: 0.0001, \\ Hidden Layers: (128, 128, 128), \\ Learning Rate: adaptive, Solver: adam\end{tabular} & 0.8668 & 0.0119 \\
        5 & \begin{tabular}[c]{@{}l@{}}Activation: tanh, Alpha: 0.001, \\ Hidden Layers: (128, 128, 128), \\ Learning Rate: constant, Solver: sgd\end{tabular} & 0.8534 & 0.0236 \\
        \bottomrule
    \end{tabular}
    \caption{Top 5 Hyperparameter Combinations}
    \label{tab:hyperparameters}
\end{table}

\end{document}
